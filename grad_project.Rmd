---
title: "Grad Project"
author: "Emily Lines"
date: "4/20/2020"
output: 
  html_document: 
    theme: cosmo
---

### Prediction Question

Can we predict whether or not a DI men's basketball team will make the NCAA tournament based off of the team's stats from that year?

### Fitting the Model


```{r install}
install.packages("ranger")
```


```{r load data and packages, warning = FASE, message = FALSE}
library(tidymodels)
library(tidyverse)
library(rpart)

ncaa_old <- read_csv("cbb.csv")
ncaa <- ncaa_old %>%
  mutate(tournament = ifelse(ncaa_old$POSTSEASON %in% 
                  c('2ND', 'Champions', 'E8', 'F4', 'R64', 'R32', 'S16'), 1, 0))
ncaa$POSTSEASON = NULL
ncaa$SEED = NULL

```

Here we load the dataset, (from Kaggle https://www.kaggle.com/andrewsundberg/college-basketball-dataset), which is DI men's basketball team stats from 2015-2019. There are 24 variables and 1757 observations. In the `POSTSEASON` column, it lists which round of the tournament the team was eliminated in, but we are only interested in whether or not the team made the tournament, so I changed that to a dummy variable with 1 representing the team making the tournament. I also deleted the column that had data on which seed the team was in the tournament because we are not interested in this.


```{r fix columns}
names(ncaa) <- c("team", "conference", "games", "wins", "adjoe", "adjde", "pwrrating", "efg_o", "efg_d", "tor", "tord", "orb", "drb", "ftr","ftrd", "fg_o", "fg_d", "threep_o", "threep_d", "adj_t", "wab", "year", "tournament")
```

I changed the column names to make the dataset easier to work with.

```{r fix variable types}
ncaa$conference <- factor(ncaa$conference)
ncaa$year <- factor(ncaa$year)
ncaa$tournament <- factor(ncaa$tournament)
levels(ncaa$tournament) <- c("no", "yes")

```

Here I changed the categorical variables into factors. 

```{r cv and spec}
ncaa_cv <- vfold_cv(ncaa, v = 5)

tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = 10,
  mode = "classification") %>%
  set_engine("rpart")

grid <- expand_grid(cost_complexity = seq(0.01, 0.05, by = 0.01))
model <- tune_grid(tree_spec,
                   tournament ~ conference + games + wins + adjoe + adjde + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
                   grid = grid,
                   resamples = ncaa_cv,
                   metrics = metric_set(gain_capture, accuracy))
```

Now we perform cross validation on our dataset and fit a decision tree. We tune the cost complexity of the tree. 


```{r class tree}
best <- model %>%
  select_best(metric = "gain_capture") %>%
  pull()

final_spec <- decision_tree(
  cost_complexity = best,
  tree_depth = 10,
  mode = "classification"
) %>%
  set_engine("rpart")

final_model <- fit(final_spec,
                   tournament ~ conference + games + wins + adjoe + adjde + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
                   data = ncaa)
final_model %>%
  predict(new_data = ncaa) %>%
  bind_cols(ncaa) %>%
  conf_mat(truth = tournament, estimate = .pred_class) %>%
  autoplot(type = "heatmap")


```

We pull out the best metric for cost complexity and use it to fit our final model. We can see that our model performed well: 95.7% of teams were correctly picked for not making the tournament and 88.9% of teams were correctly picked for making the tournament. Overall, 94.5% accuracy. 

To see if we can get better accuracy, let's try a random forest model.

```{r rf split}
set.seed(77)
ncaa_split <- initial_split(ncaa, prop = 0.5)
ncaa_train <- training(ncaa_split)

model_spec <- rand_forest(
  mode = "classification",
  mtry = 4
) %>%
  set_engine("ranger")

model2 <- fit(model_spec,
              tournament ~ conference + games + wins + adjoe + adjde + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
              data = ncaa_train)
ncaa_test <- testing(ncaa_split)
model2 %>%
  predict(new_data = ncaa_test) %>%
  bind_cols(ncaa_test) %>%
  conf_mat(truth = tournament, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

We can see that our random forest model also performed well. 94.3% of teams were accurately predicted for not making the tournament and 79.8% of teams were accurately predicted for making the tournament. Overall we had 92.3% accuracy.



```{r boosted}
set.seed(77)
boost_spec <- boost_tree(
  mode = "classification",
  tree_depth = 1,
  trees = 500,
  learn_rate = 0.001,
) %>%
  set_engine("xgboost")

model3 <- fit(boost_spec,
              tournament ~ conference + games + wins + adjoe + adjde + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
              data = ncaa_train)
ncaa_test <- testing(ncaa_split)
model3 %>%
  predict(new_data = ncaa_test) %>%
  bind_cols(ncaa_test) %>%
  conf_mat(truth = tournament, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

```{r plot packages}
install.packages("rpart.plot")
library(rpart.plot)

```

```{r plot}

best <- model %>%
  select_best(metric = "gain_capture") %>%
  pull()

final_spec <- decision_tree(
  cost_complexity = best,
  tree_depth = 10,
  mode = "classification"
) %>%
  set_engine("rpart")

final_model <- fit(final_spec,
                   tournament ~ conference + games + wins + adjoe + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
                   data = ncaa)


rpart.plot(final_model$fit,
           roundint = FALSE)

```







