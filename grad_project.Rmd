---
title: "Grad Project"
author: "Emily Lines"
date: "4/20/2020"
output:
  html_document:
    theme: cosmo
  pdf_document: default
---

### Prediction Question

Can we predict whether or not a DI men's basketball team will make the NCAA tournament based off of the team's stats from that year?

### Fitting the Model


```{r install}
install.packages("ranger")
```


```{r load data and packages, warning = FASE, message = FALSE}
library(tidymodels)
library(tidyverse)
library(rpart)

ncaa_old <- read_csv("cbb.csv")
ncaa <- ncaa_old %>%
  mutate(tournament = ifelse(ncaa_old$POSTSEASON %in% 
                  c('2ND', 'Champions', 'E8', 'F4', 'R64', 'R32', 'S16'), 1, 0))
#ncaa$POSTSEASON = NULL
#ncaa$SEED = NULL

```

Here we load the dataset, (from Kaggle https://www.kaggle.com/andrewsundberg/college-basketball-dataset), which is DI men's basketball team stats from 2015-2019. There are 24 variables and 1757 observations. In the `POSTSEASON` column, it lists which round of the tournament the team was eliminated in, but we are only interested in whether or not the team made the tournament, so I changed that to a dummy variable with 1 representing the team making the tournament. I also deleted the column that had data on which seed the team was in the tournament because we are not interested in this.


```{r fix columns}
names(ncaa) <- c("team", "conference", "games", "wins", "adjoe", "adjde", "pwrrating", "efg_o", "efg_d", "tor", "tord", "orb", "drb", "ftr","ftrd", "fg_o", "fg_d", "threep_o", "threep_d", "adj_t", "wab", "year", "tournament")
```

I changed the column names to make the dataset easier to work with.

```{r fix variable types}
ncaa$conference <- factor(ncaa$conference)
ncaa$year <- factor(ncaa$year)
ncaa$tournament <- factor(ncaa$tournament)
levels(ncaa$tournament) <- c("no", "yes")

```

Here I changed the categorical variables into factors. 

```{r cv and spec}
ncaa_cv <- vfold_cv(ncaa, v = 5)

tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = 10,
  mode = "classification") %>%
  set_engine("rpart")

grid <- expand_grid(cost_complexity = seq(0.01, 0.05, by = 0.01))
model <- tune_grid(tree_spec,
                   tournament ~ conference + games + wins + adjoe + adjde + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
                   grid = grid,
                   resamples = ncaa_cv,
                   metrics = metric_set(gain_capture, accuracy))
```

Now we perform cross validation on our dataset and fit a decision tree. We tune the cost complexity of the tree. 


```{r class tree}
best <- model %>%
  select_best(metric = "gain_capture") %>%
  pull()

final_spec <- decision_tree(
  cost_complexity = best,
  tree_depth = 10,
  mode = "classification"
) %>%
  set_engine("rpart")

final_model <- fit(final_spec,
                   tournament ~ conference + games + wins + adjoe + adjde + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
                   data = ncaa)
final_model %>%
  predict(new_data = ncaa) %>%
  bind_cols(ncaa) %>%
  conf_mat(truth = tournament, estimate = .pred_class) %>%
  autoplot(type = "heatmap")


```

We pull out the best metric for cost complexity and use it to fit our final model. We can see that our model performed well: 95.7% of teams were correctly picked for not making the tournament and 88.9% of teams were correctly picked for making the tournament. Overall, 94.5% accuracy. 

To see if we can get better accuracy, let's try a random forest model.

```{r rf split}
set.seed(77)
ncaa_split <- initial_split(ncaa, prop = 0.5)
ncaa_train <- training(ncaa_split)

model_spec <- rand_forest(
  mode = "classification",
  mtry = 4
) %>%
  set_engine("ranger")

model2 <- fit(model_spec,
              tournament ~ conference + games + wins + adjoe + adjde + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
              data = ncaa_train)
ncaa_test <- testing(ncaa_split)
model2 %>%
  predict(new_data = ncaa_test) %>%
  bind_cols(ncaa_test) %>%
  conf_mat(truth = tournament, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

We can see that our random forest model also performed well. 94.3% of teams were accurately predicted for not making the tournament and 79.8% of teams were accurately predicted for making the tournament. Overall we had 92.3% accuracy.

Now, we check the boosted tree model.

```{r boosted}
set.seed(77)
boost_spec <- boost_tree(
  mode = "classification",
  tree_depth = 1,
  trees = 500,
  learn_rate = 0.001,
) %>%
  set_engine("xgboost")

model3 <- fit(boost_spec,
              tournament ~ conference + games + wins + adjoe + adjde + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
              data = ncaa_train)
ncaa_test <- testing(ncaa_split)
model3 %>%
  predict(new_data = ncaa_test) %>%
  bind_cols(ncaa_test) %>%
  conf_mat(truth = tournament, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

For the boosted tree, we see that for not making the tournament, we were 93.5% accurate and for making the tournament we were 82.3% accurate. Overall we had 92.1% accuracy.


### Evaluating the Model

Overall, we can see that the normal classification tree performed better than the random forest or boosted tree models. So, we choose this model and conclude that we are able to predict whether or not a team will make the NCAA tournament with 94.5% accuracy. 

Now, we can plot our decision tree. 


```{r plot packages}
install.packages("rpart.plot")
library(rpart.plot)

```

```{r plot}

best <- model %>%
  select_best(metric = "gain_capture") %>%
  pull()

final_spec <- decision_tree(
  cost_complexity = best,
  tree_depth = 10,
  mode = "classification"
) %>%
  set_engine("rpart")

final_model <- fit(final_spec,
                   tournament ~ conference + games + wins + adjoe + pwrrating + efg_o + efg_d + tor + tord + orb +drb + ftr + ftrd + fg_o + fg_d + threep_o + threep_d + adj_t + wab + year,
                   data = ncaa)


rpart.plot(final_model$fit,
           roundint = FALSE)

```


In conclusion, we were able to predict whether or not a team was able to make the NCAA tournament with 94% accuracy. In order to get this result, I decided to use decision trees. I thought this would be the best method because of the simplicity of the interpretation of the models, especially with classification. I first did a regular classification decision tree, then decided to create a random forest model and boosted tree model to try to get better accuracy. But, it turns out that the original model performed the best. 

The result we obtained for this model could be very interesting when applied to 2020, this past basketball season. Since the NCAA tournament had to be cancelled due to the coronavirus, we will never know who would have actually made the tournament. But this model could be a fairly accurate way to predict who might have!



